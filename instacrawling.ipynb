{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ① 크롬 브라우저 열기\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# 인스타그램 접속하기\n",
    "driver.get('http://www.instargram.com')\n",
    "time.sleep(3)\n",
    "\n",
    "######## 인스타 계정 로그인이 필요합니다 #########\n",
    "# 예제 4-2 selenium으로 URL 접속하기 - 2\n",
    "\n",
    "email = '계정 아이디' \n",
    "input_id = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[0]\n",
    "input_id.clear()\n",
    "input_id.send_keys(email)\n",
    "\n",
    "password = '계정 비밀번호' \n",
    "input_pw = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[1]\n",
    "input_pw.clear()\n",
    "input_pw.send_keys(password)\n",
    "input_pw.submit()\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instagramfunc(word, target):\n",
    "    def insta_searching(word):\n",
    "        url = \"http://www.instagram.com/explore/tags/\" + word\n",
    "        return url\n",
    "    \n",
    "    url = insta_searching(word)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 예제 4-3 HTML에서 첫번째 게시글 찾아 클릭하기\n",
    "    def select_first(driver):\n",
    "        first = driver.find_element_by_css_selector(\"div._9AhH0\")\n",
    "        first.click()\n",
    "        time.sleep(3)\n",
    "    select_first(driver)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "     \n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    def get_content(driver):\n",
    "        # ① 현재 페이지 html 정보 가져오기\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # ② 본문 내용 가져오기\n",
    "        try:\n",
    "            content = soup.select('div.C4VMK > span')[0].text\n",
    "        except:\n",
    "            content = ''\n",
    "\n",
    "        # ③ 본문 내용에서 해시태그 가져오기(정규식 활용)\n",
    "        tags = re.findall(r'#[^\\s#,\\\\]+', content)  \n",
    "\n",
    "        # 댓글 크롤링....\n",
    "        print('댓글')\n",
    "        try:\n",
    "            comment = soup.select('div.C4VMK > span')[1].text\n",
    "        except:\n",
    "            comment = ''\n",
    "        print(comment)\n",
    "\n",
    "        ctags = re.findall(r'#[^\\s#,\\\\]+', comment)\n",
    "\n",
    "\n",
    "        # 본문 + 댓글 태그 합치기 / 중복 제거\n",
    "        TAG = tags + ctags\n",
    "        TAG = set(TAG)\n",
    "        TAG = list(TAG)\n",
    "        print(TAG)\n",
    "\n",
    "        # ⑦ 수집한 정보 저장하기\n",
    "        data = [TAG]\n",
    "        return data\n",
    "\n",
    "    # get_content(driver)\n",
    "\n",
    "    def move_next(driver,sleeptime):\n",
    "        try:\n",
    "            right = driver.find_element_by_css_selector ('a.coreSpriteRightPaginationArrow')\n",
    "            right.click()\n",
    "            time.sleep(sleeptime)\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    # move_next(driver)\n",
    "   \n",
    "    import random\n",
    "    for i in range(target):\n",
    "        if i < 500:\n",
    "            sleeptime = random.randrange(6, 14) / 3\n",
    "        elif i < 1000:\n",
    "            sleeptime = random.randrange(8, 23) / 4\n",
    "        elif i < 1500:\n",
    "            sleeptime = random.randrange(6, 20) / 3\n",
    "        elif i < 2000:\n",
    "            sleeptime = random.randrange(8, 23) / 4\n",
    "        elif i < 2500:\n",
    "            sleeptime = random.randrange(10, 32) / 5\n",
    "        elif i < 3000:\n",
    "            sleeptime = random.randrange(6, 14) / 3\n",
    "        \n",
    "        print(i, ' ', sleeptime)\n",
    "        # 게시글 수집에 오류 발생시(네트워크 문제 등의 이유로)  2초 대기 후, 다음 게시글로 넘어가도록 try, except 구문 활용\n",
    "        try:\n",
    "            data = get_content(driver)    # 게시글 정보 가져오기\n",
    "            results.append(data)\n",
    "            move_next(driver,sleeptime)\n",
    "        except:\n",
    "            time.sleep(sleeptime)\n",
    "            move_next(driver)\n",
    "\n",
    "\n",
    "    print(results[:2])\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.columns = ['tag']\n",
    "    results_df.to_csv('travel_ex.csv', encoding='utf-8')\n",
    "\n",
    "    # 예제 4-8 여러 개의 저장파일 통합하기\n",
    "    travel_insta_df = pd.DataFrame( [ ] )\n",
    "\n",
    "    #folder = './files/'\n",
    "    f_list = ['travel_ex.csv']\n",
    "    for fname in f_list:\n",
    "        fpath =  fname\n",
    "        temp = pd.read_csv(fpath)\n",
    "        travel_insta_df = travel_insta_df.append(temp)\n",
    "\n",
    "    travel_insta_df.columns =['scrollnum', 'tag']\n",
    "\n",
    "    # 예제 4-9 중복 데이터 제거하고 저장하기\n",
    "    #travel_insta_df.drop_duplicates(subset = ['content'] , inplace = True)\n",
    "    travel_insta_df.to_csv('insta_{}.csv', index = False, encoding = 'utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
